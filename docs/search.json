[
  {
    "objectID": "model_page.html",
    "href": "model_page.html",
    "title": "Web Sales Prediction",
    "section": "",
    "text": "Feature Engineering\nFeature Scaling will be performed in order to transform numerical attributes to be on similar scales.\nSince most Machine Learning algorithms prefer to work with numbers, we will convert the customer segment categories from text to numbers.\n\nm_res <- recipe(sales_per_visit ~ ., data = train) |>\n  step_normalize(all_numeric_predictors()) |>\n  step_dummy(all_nominal_predictors())\n\nm_res\n\nRecipe\n\nInputs:\n\n      role #variables\n   outcome          1\n predictor          7\n\nOperations:\n\nCentering and scaling for all_numeric_predictors()\nDummy variables from all_nominal_predictors()\n\n\n\n\nModel\nThree model will be trained and evaluated to see which of them will have the best potential at generalizing well on the test set this models are linear regression, decision tree and a random forest model.\n\nlr_mdl <- linear_reg() |>\n  set_engine(\"lm\")\n\ndt_mdl <- decision_tree() |>\n  set_engine(\"rpart\") |>\n  set_mode(\"regression\")\n\nrf_mdl <- rand_forest() |>\n  set_engine(\"ranger\") |>\n  set_mode(\"regression\")\n\n\n\nWorkflow\nCreating a workflow pipeline which combines both feature engineering and the model specification together. this will be done for each model.\n\nlr_wf <- workflow() |>\n  add_recipe(m_res) |>\n  add_model(lr_mdl)\n\ndt_wf <- workflow() |>\n  add_recipe(m_res) |>\n  add_model(dt_mdl)\n\nrf_wf <- workflow() |>\n  add_recipe(m_res) |>\n  add_model(rf_mdl)\n\n\n\nCross Validation\nIn order to better evaluate the model performance the training set will be divided further into smaller training set and a validation set, the models will then be trained against the smaller training set and evaluate them against the validation set. Note that there will be 10 separate folds (train set & validation set) and split will be done using a stratified sampling based on the newly created customer segment variable so that each set will have a random sample close to that of the population (the entire train set)\n\nset.seed(1122)\n\nv_folds <- vfold_cv(train, v = 10, strata = \"customer_segment\")\n\n\nLinear Regression\n\ndoParallel::registerDoParallel()\nlr_fit <- lr_wf |>\n  fit_resamples(resamples = v_folds,\n                metrics = metric_set(rmse, rsq, mae),\n                control = control_resamples(save_pred = TRUE, verbose = FALSE))\n\ncollect_metrics(lr_fit) |> cross_valid_tibble(model = \"default\")\n\n\n\n\n\n\n\n\n\nDecision Tree\n\ndt_fit <- dt_wf |>\n  fit_resamples(resamples = v_folds,\n                metrics = metric_set(rmse, rsq, mae),\n                control = control_resamples(save_pred = TRUE, verbose = FALSE))\n\ncollect_metrics(dt_fit) |> cross_valid_tibble(model = \"default\")\n\n\n\n\n\n\n\n\n\nRandom Forest\n\nrf_fit <- rf_wf |>\n  fit_resamples(resamples = v_folds,\n                metrics = metric_set(rmse, rsq, mae),\n                control = control_resamples(save_pred = TRUE, verbose = FALSE))\n\ncollect_metrics(rf_fit) |> cross_valid_tibble(model = \"default\")\n\n\n\n\n\n\n\nEvaluating the performance of the three models we can see that the random forest model made the least error when predicting sales per visit with RMSE of 46.0 and MAE of 22.2 way better than decision tree with RMSE of 58.1 and MAE of 34.8 linear regression performed worst of all three models.\n\n\n\nhyper-parameter tuning\nThe model performance can be improved further by feeding the best parameter to the model. A grid search will be used to save time and reduce errors.\n\nCreating a new tunable model for both decision tree and random forest model.\n\n\nDecision Tree\nExperimenting on three decision tree hyperparameters which are::\ncost complexity :- the cost parameter (cp) used by CART model.\ntree depth :- The maximum depth of the decision tree.\nmin n :- The minimum number of data points required for a node to be split further.\n\ndt_t_mdl <- decision_tree(cost_complexity = tune(),\n                          tree_depth = tune(),\n                          min_n = tune()) |>\n  set_engine(\"rpart\") |>\n  set_mode(\"regression\")\n\n# Update workflow model\ndt_wf <- update_model(dt_wf, dt_t_mdl)\n\n\nset.seed(1221)\ndoParallel::registerDoParallel()\ndt_tune <- tune_grid(dt_wf,\n                     resamples = v_folds,\n                     grid = 10,\n                     metrics = metric_set(mae))\n\n\nGrid PerfomanceBest ParametersPlot\n\n\n\ncollect_metrics(dt_tune) |> cross_valid_tibble(\"dt\")\n\n\n\n\n\n\n\n\n\nshow_best(dt_tune, metric = \"mae\") |> cross_valid_tibble(\"dt\")\n\n\n\n\n\n\n\n\n\nautoplot(dt_tune) + theme_minimal()\n\n\n\n\n\n\n\nBased on the average MAE of all 10 split there is an increase in the model performance of which there is a reduction in the prediction error from 33.4 down to 20.1. let’s see how the random forest model performs.\n\n\n\nRandom Forest\nExperimenting on two random forest hyperparameters which are::\nmtry :- Number of predictors that will be randomly sampled at each split when creating the tree model.\nmin_n :- The minimum number of data points required for a node to be split further.\n\nrf_t_mdl <- rand_forest(mtry = tune(),\n                        trees = 1000,\n                        min_n = tune()) |>\n  set_engine(\"ranger\") |>\n  set_mode(\"regression\")\n\n# Update workflow model\nrf_wf <- update_model(rf_wf, rf_t_mdl)\n\n\nset.seed(1221)\ndoParallel::registerDoParallel()\nrf_tune <- tune_grid(rf_wf,\n                     resamples = v_folds,\n                     grid = 5,\n                     metrics = metric_set(mae))\n\ni Creating pre-processing data to finalize unknown parameter: mtry\n\n\n\nGrid PerfomanceBest ParametersPlot\n\n\n\ncollect_metrics(rf_tune) |> cross_valid_tibble(\"rf\")\n\n\n\n\n\n\n\n\n\nshow_best(rf_tune, metric = \"mae\") |> cross_valid_tibble(\"rf\")\n\n\n\n\n\n\n\n\n\nautoplot(rf_tune) + theme_minimal()\n\n\n\n\n\n\n\nThe tunned random forest model have the best performance so far, with the lowest error of 20.4 on an average for all 10 split.\n\n\n\n\nFeature Importance\n\nimp_spec <- rf_mdl |>\n  finalize_model(select_best(rf_tune, metric = \"mae\")) |>\n  set_engine(\"ranger\", importance = \"permutation\")\n\nworkflow() |>\n  add_recipe(m_res) |>\n  add_model(imp_spec) |>\n  fit(train) |>\n  extract_fit_parsnip() |>\n  vip(aesthetics = list(alpha = 0.9, fill = \"#D3AB9E\")) +\n  ggtitle(\"Feature Importance\") +\n  scale_y_continuous(labels = scales::label_comma()) +\n  theme_minimal() +\n  theme(plot.title = element_text(color = \"#888888\"),\n        axis.title.x = element_text(color = \"#8F8F8F\"),\n        axis.text = element_text(color = \"#8F8F8F\"))\n\n\n\n\nThe number of visit from customers which involved a least a purchase is the most important variable when predicting sales per visit which is closely followed by the number of unique items purchased while the dummy variables for customer segment have the smallest contribution when predicting sales per visit.\n\n\nFinalize workflow\nGoing forward the random forest will be used to predict sales per visit on the test set to see how well the model generalizes.\n\nLoad Testing data\n\ntest <- readr::read_delim(\"data/clothing_store_PCA_test\", delim = \",\") |>\n  clean_add_variable()\n\n\nfinal_rf <- rf_wf |>\n  finalize_workflow(select_best(rf_tune, metric = \"mae\"))\n\n\n\n\nFit & Predict\n\n# Clean and add variables to the test set\ntest <- clean_add_variable(test)\n\nfit_workflow <- fit(final_rf, train)\n\nsales_prediction <- predict(fit_workflow, test)\n\npred_df <- sales_prediction |>\n  mutate(actual = test$sales_per_visit)\n\n\n\nEvaluation\n\nmae(pred_df, truth = actual, estimate = .pred) |>\n  reactable(\n    theme = reactableTheme(color = \"#919191\",\n                           headerStyle = list(color = \"#858585\")),\n    columns = list(.estimate = colDef(format = colFormat(digits = 3))),\n    bordered = TRUE\n  )\n\n\n\n\n\n\n\nnumeric_summary(pred_df, .pred, FALSE) |>\n  mutate(variable = \"Prediction\") |>\n  bind_rows(numeric_summary(pred_df, actual, FALSE) |> mutate(variable = \"Actual\")) |>\n  relocate(variable, .before = 1) |>\n  mutate(across(where(is.numeric), ~round(.x, 2))) |>\n  rename_with(axis_label) |>\n\n  reactable(\n    theme = reactableTheme(color = \"#919191\",\n                           headerStyle = list(color = \"#858585\")),\n    defaultColDef = colDef(format = colFormat(separators = TRUE,\n                                              prefix = \"$\")),\n    columns = list(\n      Variable = colDef(style = list(color = \"#777777\",\n                                     fontWeight = \"bold\"),\n                        format = colFormat(prefix = NULL))\n    )\n  )\n\n\n\n\n\n\n\npred_df |>\n  ggplot(aes(actual, .pred)) +\n  geom_abline(lty = 2, color = \"#D3AB9E\") +\n  geom_point(color = \"#D3AB9E\") +\n  scale_x_continuous(labels = scales::label_dollar()) +\n  scale_y_continuous(labels = scales::label_dollar()) +\n  theme_minimal() +\n  labs(x = \"Actual\", y = \"Prediction\",\n       title = \"Actual Vs Predicted Sales Per Visit\") +\n  theme(axis.title = element_text(color = \"#5E5E5E\"),\n        plot.title = element_text(color = \"#5E5E5E\"))\n\n\n\n\nPrediction on the test set have a Mean Absolute Error of 20.5 which mean the final random forest model is not over fitting the data. Also from the scatter plot above there is a certain pattern of error for the actual and predicted sales per visit which shows smaller errors for sales below $300 and a higher error for sales above that amount. Another obvious error the model made is failing to predict sales per visit above $716.9.\n\n\nSave Final Model\n\nmdl_v <- vetiver::vetiver_model(model = fit_workflow,\n                                model_name = \"sales_random_forest\")\n\nmodel_board <- pins::board_folder(path = \"model_workflow\", versioned = TRUE)\n\nvetiver::vetiver_pin_write(model_board, mdl_v)\n\n\nFunction for future prediction of sales per visit.\n\n#' Predict Sales per visit on new set of records.\n#'\n#' @param board_path path to the saved pin board.\n#' @param mdl_version The model version.\n#' @param new_data  A new data set to predict.\n#'\n#' @return A tibble with a single variable \".pred\" which holds the new predictions of sales per visit.\n#' @export\n#'\n#' @examples predict_sale_per_visit(\"~/sales_prediction/board\", new_web_records)\n#' \npredict_sale_per_visit <- function(board_path, mdl_version, new_data) {\n  model_board <- pins::board_folder(path = board_path)\n  \n  vers <- if (!missing(mdl_version)) mdl_version else NULL\n  \n  wf_model <- vetiver::vetiver_pin_read(board = model_board, \n                                        name = \"sales_random_forest\",\n                                        version = vers)\n  \n  library(workflows)\n  \n  predict(wf_model, new_data)\n}"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Akinwande Ayomide",
    "section": "",
    "text": "Github\n  \n  \n    \n     Twitter\n  \n  \n    \n     Linkedin\n  \n\n      \nA strategic data analyst with a (passion) for developing (operational process) that (solves) business problems, improves business efficiency and delivering actionable insights using (various) analytical tools.\n\n\nAdekunle Ajasin University Akungba, Ondo State | Msc in Banking & Finance | Sept 2015 - Jan 2019\n\n\n\n\n\n\n\n\nPM2.5 sensor device offset Prediction.\n\n\nHospitality Management Dashboard.\n\n\nSegmentation Analysis Applications.\n\n\nMarket Basket Analysis Application."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data & Exploration",
    "section": "",
    "text": "An online clothing store has decided to take their marketing strategy to the next level by taking advantage of the available customer activity data. Their aim is to predict potential sales of various collection of high quality fashion brands which will in turn help to reduce marketing cost by making sure that they are spending more on niches and items with high earning potential by offering not only the best prices but also taking advantage of various event, holidays, etc."
  },
  {
    "objectID": "index.html#data",
    "href": "index.html#data",
    "title": "Data & Exploration",
    "section": "Data",
    "text": "Data\n\ntrain <- read_delim(\"data/clothing_store_PCA_training\", delim = \",\")\n\ndisplay_tibble(head(train, 5))\n\n\n\n\n\n\n\nVariable Definition\ndays since purchase :: Number of days after a purchase was made.\npurchase visits :: Number of visit that ended with a purchase.\ndays in file :: Total number of days starting from the first purchase date.\ndays between purchases :: Average number of days between purchases.\ndiff items purchased :: The total number of unique items purchased.\nsales per visit :: Average sales per customer visit.\n\n\nData Inspection\n\nskimr::skim(train)\n\n\nData summary\n\n\nName\ntrain\n\n\nNumber of rows\n14602\n\n\nNumber of columns\n6\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nnumeric\n6\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nDays since Purchase\n0\n1\n126.16\n104.22\n1.00\n35.00\n97.00\n203.00\n365.00\n▇▅▂▂▂\n\n\nPurchase Visits\n0\n1\n5.04\n6.39\n1.00\n1.00\n3.00\n6.00\n115.00\n▇▁▁▁▁\n\n\nDays on File\n0\n1\n437.88\n193.03\n4.00\n286.00\n448.00\n630.00\n713.00\n▂▃▅▅▇\n\n\nDays between Purchases\n0\n1\n172.26\n148.70\n4.00\n66.81\n124.00\n231.92\n713.00\n▇▃▂▁▁\n\n\nDiff Items Purchased\n0\n1\n17.20\n25.68\n1.00\n5.00\n9.00\n20.00\n743.00\n▇▁▁▁▁\n\n\nSales per Visit\n0\n1\n114.14\n87.95\n2.17\n60.79\n92.02\n139.62\n1919.88\n▇▁▁▁▁\n\n\n\n\n\nThere are no missing values in the data (n_missing column) and the average sales per visit is 114.14 which is mostly influenced by the outliers in the data (inline histogram & p50).\n\n\nAdding Useful variables\n\ntrain <- clean_add_variable(train) \n\ndisplay_tibble(head(train, 5))"
  },
  {
    "objectID": "index.html#data-exploration",
    "href": "index.html#data-exploration",
    "title": "Data & Exploration",
    "section": "Data Exploration",
    "text": "Data Exploration\n\nVariable Distribution\n\nSales Per visitLast PurchasePurchase VisitLength Of TimeUnique Purchased Items\n\n\n\n\n\n\n\nMajority of customers spent less than $400 for customer visits and there is a huge outlier close to $2,000. Given this information the MAE will be more preferred for model evaluation since it does not penalize high error caused by outliers unlike RMSE, in other words all error are weighted on the same scale.\n\n\n\n\n\n\n\nThe right skewed plot shows that more customers have made purchases recently with a minimum of 1 day and an average of 126. days of course it was influenced by some large outliers by customers who have not made any purchase recently.\n\n\n\n\n\n\n\nThe number of visit to the online clothing store which lead to at least a purchase shows that majority on an average made a purchase at least 5 times on different visits.\n\n\n\n\n\n\n\nThe number of days customers have spent with the store distribution show that majority have spent more time with the business with a maximum of 713 days (almost 2 years) and an average of 438 days.\n\n\n\n\n\n\n\nThe median value of unique number of items purchased by customers is 9 items while them maximum is 743 different items which can be somewhat of a bulk purchase.\n\n\n\n\n\nVariable Relationship\n\nSales Per visitPurchase VisitUnique Purchased ItemsLast Purchase\n\n\n\n\n\n\n\nThe relationship between sales per visit and days since purchase, days on file and days between purchases dose not show any obvious pattern except that there are more low amount of sales than high amount.\nWhile for purchase visit the same pattern as the sales per visit distribution can be seen in the scatter plot.\n\n\n\n\n\n\n\nThere above scatter plots show that:: 1. As the time span since the last purchase increases the number of purchase visit decreases. 2. There is a positive relationship between purchase visit and the number of unique items purchased.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCustomer Segments\n\ntrain |>\n  count(customer_segment, sort = TRUE, name = \"count\") |>\n  mutate(percentage = round(proportions(count)*100, 2)) |>\n  \n  ggplot(aes(x = fct_rev(fct_reorder(customer_segment, count)), y = count)) +\n  geom_col(fill = \"#D3AB9E\") +\n  geom_text(aes(label = glue::glue(\"{percentage}%\"), vjust = 2),\n            color = \"#4A4A4A\") +\n  scale_y_continuous(labels = scales::label_comma()) +\n  labs(x = NULL, y = NULL) +\n  ggtitle(\"Number of Customer In Each Segment\") +\n  theme_minimal() +\n  theme(plot.title = element_text(color = \"#888888\"),\n        axis.text = element_text(color = \"#919191\"),\n        plot.background = element_rect(fill = \"#FFFBFF\",\n                                       color = \"#FFFBFF\"))\n\n\n\n\n\n\nCustomer Segment & Variable Aggregate summary\n\n\n\n\n\n\n\n\n\nCorrelation\n\ntrain |> \n  rename_with(axis_label) |>\n  select(where(is.numeric)) |>\n  cor() |>\n  reshape2::melt() |> \n  ggplot(aes(Var1, Var2, fill = value)) +\n  geom_tile(height = 0.8, width = 0.8) +\n  geom_text(aes(label = round(value, 2)), size = 3, color = \"#000000\") +\n  scale_fill_gradient2(low = \"#E39774\", mid = \"#EBD8D0\", high = \"#8B786D\") +\n  theme_minimal() +\n  coord_equal() +\n  labs(x = NULL, y = NULL) +\n  theme(axis.text.x = element_text(size = 8, \n                                   margin = margin(-3, 0, 0, 0),\n                                   angle = 25, \n                                   hjust = 1),\n        axis.text.y = element_text(size = 8, \n                                   margin = margin(0, -3, 0, 0)),\n        panel.grid.major = element_blank())\n\n\n\n\nThe number of unique items purchased is highly correlated with purchase visits also days between purchase is moderately correlated with number of days since purchase."
  }
]